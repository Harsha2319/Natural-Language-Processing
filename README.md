# Natural-Language-Processing - Mini Projects

#### 1. Smoothing Techniques
   Builds a bigram model based on the training data and Computes probability of the test sentence
        * without Smoothing
        * with Add-One Smoothing
        * with Good Turing Smoothing
        
#### 2. Probabilistic POS Tagger
   Builds a bigram model based on the training data and performs Na√Øve Bayesian Classification (Bigram) based POS Tagging
    
#### 3. HMM Decoding - Viterbi Algorithm
   Computes probability of the test sentence using Viterbi Algorithm given the HMM Transition Probability and HMM Observation Likelihood
