# Natural-Language-Processing - Mini Projects

<br/>
## 1. Smoothing Techniques
Builds a bigram model based on the training data and Computes probability of the test sentence:
* without Smoothing
* with Add-One Smoothing
* with Good Turing Smoothing
 
<br/> 
## 2. Probabilistic POS Tagger
Builds a bigram model based on the training data and performs Na√Øve Bayesian Classification (Bigram) based POS Tagging
   
<br/>   
## 3. HMM Decoding - Viterbi Algorithm
Computes probability of the test sentence using Viterbi Algorithm given the HMM Transition Probability and HMM Observation Likelihood
